# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iW2TmWU1WS6W9ewUEgZkRysJd-jm58NZ
"""

"""
NewsBot Intelligence System - Complete Implementation
Mid-Term Project: NLP Pipeline for News Article Analysis
Author: Saima Sano
Date: November 1st, 2025

NOTE: This version loads dataset from GitHub repository
"""

# ============================================================================
# INSTALLATION AND IMPORTS
# ============================================================================

print("üì¶ Installing required packages...")
# Install required packages
!pip install -q spacy scikit-learn nltk textblob
!python -m spacy download en_core_web_sm

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# NLP Libraries
import spacy
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from textblob import TextBlob

# Download NLTK data
print("\nüì• Downloading NLTK data...")
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)
nltk.download('omw-1.4', quiet=True)
# Add download for punkt_tab
nltk.download('punkt_tab', quiet=True)


# Load spaCy model
print("üîß Loading spaCy model...")
nlp = spacy.load('en_core_web_sm')

print("‚úÖ All libraries installed and loaded successfully!\n")

# ============================================================================
# LOAD DATASET FROM GITHUB
# ============================================================================

print("="*80)
print("LOADING DATASET FROM GITHUB")
print("="*80)

# IMPORTANT: Replace this URL with YOUR GitHub repository raw file URL
# Format: https://raw.githubusercontent.com/saimasano123/main/NewsBot-Intelligence-System_NLP-Midterm-Project/data/bbc_news_train.csv

GITHUB_DATASET_URL = "https://raw.githubusercontent.com/saimasano123/NewsBot-Intelligence-System-NLP-Midterm-Project/refs/heads/main/bbc_news_train.csv"

# Instructions to get your GitHub raw URL:
# 1. Go to your GitHub repository
# 2. Navigate to: ITAI2373-NewsBot-Midterm/data/bbc_news_train.csv
# 3. Click the "Raw" button
# 4. Copy the URL from browser address bar
# 5. Paste it above replacing YOUR_GITHUB_RAW_URL_HERE


print(f"üìç Attempting to load dataset from GitHub...")
print(f"   URL: {GITHUB_DATASET_URL}\n")

try:
    # Try loading from GitHub
    if "YOUR_GITHUB" not in GITHUB_DATASET_URL:
        df = pd.read_csv(GITHUB_DATASET_URL)
        print(f"‚úÖ Dataset loaded successfully from GitHub!")
    else:
        print("‚ö†Ô∏è  GitHub URL not configured yet!")
        print("\nPlease upload your dataset file using the Colab file upload:")
        from google.colab import files
        uploaded = files.upload()

        # Get the uploaded filename
        filename = list(uploaded.keys())[0]
        df = pd.read_csv(filename)
        print(f"‚úÖ Dataset loaded from uploaded file: {filename}")

except Exception as e:
    print(f"‚ùå Error loading dataset: {e}")
    print("\nüîÑ Alternative: Upload file manually")
    from google.colab import files
    print("Please upload your BBC News CSV file:")
    uploaded = files.upload()
    filename = list(uploaded.keys())[0]
    df = pd.read_csv(filename)
    print(f"‚úÖ Dataset loaded from uploaded file: {filename}")

# Display dataset information
print(f"\nüìä Dataset Overview:")
print(f"   ‚Ä¢ Shape: {df.shape}")
print(f"   ‚Ä¢ Columns: {df.columns.tolist()}")

# Detect and standardize column names
print("\nüîç Detecting column names...")
text_col = None
category_col = None

# Check for text column
for col in df.columns:
    if col.lower() in ['text', 'content', 'article', 'news', 'description']:
        text_col = col
        break

# Check for category column
for col in df.columns:
    if col.lower() in ['category', 'label', 'class', 'type']:
        category_col = col
        break

if text_col and category_col:
    print(f"   ‚úì Text column: {text_col}")
    print(f"   ‚úì Category column: {category_col}")

    # Standardize column names
    df = df.rename(columns={text_col: 'content', category_col: 'category'})
else:
    print(f"   ‚ö†Ô∏è  Could not auto-detect columns")
    print(f"   Available columns: {df.columns.tolist()}")
    print("\n   Please manually specify column names:")
    print("   df = df.rename(columns={'YOUR_TEXT_COL': 'content', 'YOUR_CATEGORY_COL': 'category'})")

# Display first few rows
print("\nüìã First few rows:")
print(df.head(2))

# Check category distribution
print(f"\nüìä Category Distribution:")
print(df['category'].value_counts())

# Check for missing values
print(f"\nüîç Missing Values:")
missing = df.isnull().sum()
print(missing[missing > 0] if missing.sum() > 0 else "   ‚úì No missing values")

# Clean dataset
print("\nüßπ Cleaning dataset...")
df_original_size = len(df)
df = df.dropna(subset=['content', 'category'])
df = df[df['content'].str.len() > 50]  # Remove very short articles

print(f"   ‚Ä¢ Original: {df_original_size} articles")
print(f"   ‚Ä¢ After cleaning: {len(df)} articles")
print(f"   ‚Ä¢ Removed: {df_original_size - len(df)} articles")

# Display dataset information
print(f"\nüìä Dataset Overview:")
print(f"   ‚Ä¢ Shape: {df.shape}")
print(f"   ‚Ä¢ Columns: {df.columns.tolist()}")

# Detect and standardize column names
print("\nüîç Detecting column names...")
text_col = None
category_col = None

# Check for text column
for col in df.columns:
    if col.lower() in ['text', 'content', 'article', 'news', 'description']:
        text_col = col
        break

# Check for category column
for col in df.columns:
    if col.lower() in ['category', 'label', 'class', 'type']:
        category_col = col
        break

if text_col and category_col:
    print(f"   ‚úì Text column: {text_col}")
    print(f"   ‚úì Category column: {category_col}")

    # Standardize column names
    df = df.rename(columns={text_col: 'content', category_col: 'category'})
else:
    print(f"   ‚ö†Ô∏è  Could not auto-detect columns")
    print(f"   Available columns: {df.columns.tolist()}")
    print("\n   Please manually specify column names:")
    print("   df = df.rename(columns={'YOUR_TEXT_COL': 'content', 'YOUR_CATEGORY_COL': 'category'})")

# Display first few rows
print("\nüìã First few rows:")
print(df.head(2))

# Check category distribution
print(f"\nüìä Category Distribution:")
print(df['category'].value_counts())

# Check for missing values
print(f"\nüîç Missing Values:")
missing = df.isnull().sum()
print(missing[missing > 0] if missing.sum() > 0 else "   ‚úì No missing values")

# Clean dataset
print("\nüßπ Cleaning dataset...")
df_original_size = len(df)
df = df.dropna(subset=['content', 'category'])
df = df[df['content'].str.len() > 50]  # Remove very short articles

print(f"   ‚Ä¢ Original: {df_original_size} articles")
print(f"   ‚Ä¢ After cleaning: {len(df)} articles")
print(f"   ‚Ä¢ Removed: {df_original_size - len(df)} articles")

# ============================================================================
# MODULE 1: BUSINESS CONTEXT AND APPLICATION
# ============================================================================

print("\n" + "="*80)
print("MODULE 1: NEWSBOT INTELLIGENCE SYSTEM - BUSINESS CONTEXT")
print("="*80)

business_context = """
üéØ APPLICATION: NewsBot Intelligence System for Media Monitoring

BUSINESS CASE:
Media companies, financial institutions, and market research firms need to process
thousands of news articles daily to identify trends, monitor competitor coverage,
and track emerging stories. Manual processing is time-consuming and expensive.

TARGET USERS:
- Media analysts tracking industry trends
- Financial traders monitoring market-moving news
- PR professionals managing brand reputation
- Researchers analyzing news coverage patterns

VALUE PROPOSITION:
- Automated categorization saving 20+ hours/week per analyst
- Real-time entity extraction for relationship mapping
- Sentiment analysis for brand monitoring
- Pattern detection for trend forecasting

ROI: Reduce manual processing costs by 75% while increasing coverage by 10x
"""

print(business_context)

# ============================================================================
# MODULE 2: TEXT PREPROCESSING PIPELINE
# ============================================================================

print("\n" + "="*80)
print("MODULE 2: TEXT PREPROCESSING PIPELINE")
print("="*80)

class TextPreprocessor:
    """Comprehensive text preprocessing for news articles"""

    def __init__(self):
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))

    def clean_text(self, text):
        """Clean and normalize text"""
        if pd.isna(text):
            return ""

        # Convert to lowercase
        text = str(text).lower()

        # Remove special characters but keep sentence structure
        import re
        text = re.sub(r'[^a-zA-Z\s\.]', ' ', text)

        # Remove extra whitespace
        text = ' '.join(text.split())

        return text

    def tokenize(self, text):
        """Tokenize text"""
        return word_tokenize(text)

    def remove_stopwords(self, tokens):
        """Remove stop words"""
        return [token for token in tokens if token not in self.stop_words and len(token) > 2]

    def lemmatize(self, tokens):
        """Lemmatize tokens"""
        return [self.lemmatizer.lemmatize(token) for token in tokens]

    def preprocess(self, text):
        """Complete preprocessing pipeline"""
        # Clean
        text = self.clean_text(text)
        # Tokenize
        tokens = self.tokenize(text)
        # Remove stopwords
        tokens = self.remove_stopwords(tokens)
        # Lemmatize
        tokens = self.lemmatize(tokens)

        return ' '.join(tokens)

# Initialize preprocessor
preprocessor = TextPreprocessor()

print(f"üìä Dataset: {len(df)} articles across {len(df['category'].unique())} categories")
print(f"üìÅ Categories: {df['category'].unique().tolist()}")

# Apply preprocessing
print("\nüîÑ Preprocessing articles (this may take 1-2 minutes)...")
df['processed_text'] = df['content'].apply(preprocessor.preprocess)

# Show example
print("\nüìù Preprocessing Example:")
sample_idx = 0
print(f"\n{'='*70}")
print("ORIGINAL TEXT:")
print(f"{'='*70}")
print(df['content'].iloc[sample_idx][:300] + "...")
print(f"\n{'='*70}")
print("PROCESSED TEXT:")
print(f"{'='*70}")
print(df['processed_text'].iloc[sample_idx][:300] + "...")
print(f"{'='*70}")

# Statistics
avg_length_original = df['content'].str.len().mean()
avg_length_processed = df['processed_text'].str.len().mean()
reduction = ((avg_length_original - avg_length_processed) / avg_length_original) * 100

print(f"\nüìä Preprocessing Statistics:")
print(f"   ‚Ä¢ Average original length: {avg_length_original:.0f} characters")
print(f"   ‚Ä¢ Average processed length: {avg_length_processed:.0f} characters")
print(f"   ‚Ä¢ Text reduction: {reduction:.1f}%")

# ============================================================================
# MODULE 3: TF-IDF FEATURE EXTRACTION
# ============================================================================

print("\n" + "="*80)
print("MODULE 3: TF-IDF FEATURE EXTRACTION AND ANALYSIS")
print("="*80)

# Create TF-IDF vectorizer
print("üîß Creating TF-IDF matrix...")
tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=2, max_df=0.8)
X_tfidf = tfidf.fit_transform(df['processed_text'])

print(f"‚úÖ TF-IDF Matrix created: {X_tfidf.shape}")
print(f"   ‚Ä¢ Features: {X_tfidf.shape[1]}")
print(f"   ‚Ä¢ Documents: {X_tfidf.shape[0]}")
print(f"   ‚Ä¢ Sparsity: {(1.0 - X_tfidf.nnz / (X_tfidf.shape[0] * X_tfidf.shape[1])) * 100:.2f}%")

# Analyze top terms per category
def get_top_tfidf_terms(category, n=10):
    """Get top TF-IDF terms for a category"""
    category_indices = df[df['category'] == category].index
    category_tfidf = X_tfidf[category_indices].mean(axis=0)
    category_tfidf_array = np.asarray(category_tfidf).flatten()

    top_indices = category_tfidf_array.argsort()[-n:][::-1]
    feature_names = tfidf.get_feature_names_out()

    return [(feature_names[i], category_tfidf_array[i]) for i in top_indices]

# Display top terms for each category
print("\nüìä Top TF-IDF Terms by Category:")
for category in sorted(df['category'].unique()):
    print(f"\n{category.upper()}:")
    top_terms = get_top_tfidf_terms(category, n=10)
    for term, score in top_terms:
        print(f"  ‚Ä¢ {term:20s} {score:.4f}")

# Visualize top terms
print("\nüìä Generating TF-IDF visualization...")
categories = sorted(df['category'].unique())
n_categories = len(categories)

# Create subplot grid
if n_categories <= 4:
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
elif n_categories <= 6:
    fig, axes = plt.subplots(2, 3, figsize=(16, 10))
else:
    fig, axes = plt.subplots(3, 3, figsize=(18, 12))

axes = axes.flatten()

for idx, category in enumerate(categories):
    if idx < len(axes):
        top_terms = get_top_tfidf_terms(category, n=10)
        terms, scores = zip(*top_terms)

        axes[idx].barh(range(len(terms)), scores, color='steelblue')
        axes[idx].set_yticks(range(len(terms)))
        axes[idx].set_yticklabels(terms, fontsize=9)
        axes[idx].set_xlabel('TF-IDF Score', fontsize=10)
        axes[idx].set_title(f'{category.capitalize()}', fontsize=12, fontweight='bold')
        axes[idx].invert_yaxis()
        axes[idx].grid(axis='x', alpha=0.3)

# Hide extra subplots
for idx in range(n_categories, len(axes)):
    axes[idx].set_visible(False)

plt.tight_layout()
plt.savefig('tfidf_analysis.png', dpi=300, bbox_inches='tight')
print("üíæ Saved: tfidf_analysis.png")
plt.show()

# ============================================================================
# MODULE 4: PART-OF-SPEECH PATTERN ANALYSIS
# ============================================================================

print("\n" + "="*80)
print("MODULE 4: PART-OF-SPEECH PATTERN ANALYSIS")
print("="*80)

def analyze_pos_patterns(text):
    """Analyze POS patterns in text"""
    try:
        # Limit text length for efficiency
        text_sample = text[:1000] if len(text) > 1000 else text
        tokens = word_tokenize(text_sample.lower())
        pos_tags = nltk.pos_tag(tokens)
        return [tag for word, tag in pos_tags]
    except:
        return []

# Analyze POS distribution for sample articles
print("üîç Analyzing POS patterns (sampling 100 articles per category)...")
sample_size = min(100, len(df))
df_sample = df.groupby('category').head(20).reset_index(drop=True)

print(f"   ‚Ä¢ Analyzing {len(df_sample)} articles...")
df_sample['pos_tags'] = df_sample['content'].apply(analyze_pos_patterns)

# Calculate POS distribution by category
pos_by_category = {}
for category in df['category'].unique():
    category_data = df_sample[df_sample['category'] == category]
    all_tags = []
    for tags in category_data['pos_tags'].dropna():
        all_tags.extend(tags)
    pos_by_category[category] = Counter(all_tags)

# Display POS statistics
print("\nüìä POS Tag Counts by Category:")
common_pos = ['NN', 'NNS', 'VB', 'VBD', 'VBG', 'JJ', 'RB', 'NNP']
for category in sorted(df['category'].unique()):
    print(f"\n{category.upper()}:")
    for pos in common_pos:
        count = pos_by_category.get(category, Counter()).get(pos, 0)
        print(f"  ‚Ä¢ {pos:4s}: {count:4d}")

# Visualize POS distribution
print("\nüìä Generating POS distribution visualization...")
pos_data = []
for category in df['category'].unique():
    for pos in common_pos:
        count = pos_by_category.get(category, Counter()).get(pos, 0)
        pos_data.append({'Category': category, 'POS': pos, 'Count': count})

pos_df = pd.DataFrame(pos_data)

plt.figure(figsize=(14, 6))
for pos in common_pos:
    pos_subset = pos_df[pos_df['POS'] == pos]
    plt.plot(pos_subset['Category'], pos_subset['Count'], marker='o', label=pos, linewidth=2)

plt.xlabel('Category', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.title('POS Tag Distribution Across Categories', fontsize=14, fontweight='bold')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.xticks(rotation=45, ha='right')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('pos_analysis.png', dpi=300, bbox_inches='tight')
print("üíæ Saved: pos_analysis.png")
plt.show()

# ============================================================================
# MODULE 5: SYNTAX PARSING AND SEMANTIC ANALYSIS
# ============================================================================

print("\n" + "="*80)
print("MODULE 5: SYNTAX PARSING AND SEMANTIC ANALYSIS")
print("="*80)

def extract_dependency_patterns(text, n_samples=5):
    """Extract dependency patterns using spaCy"""
    try:
        doc = nlp(text[:1000])  # Limit text length for efficiency

        patterns = []
        for token in doc:
            if token.dep_ in ['nsubj', 'dobj', 'pobj', 'nsubjpass']:
                pattern = {
                    'text': token.text,
                    'pos': token.pos_,
                    'dep': token.dep_,
                    'head': token.head.text
                }
                patterns.append(pattern)

        return patterns[:n_samples]
    except:
        return []

# Analyze dependency patterns for sample articles
print("üîç Extracting dependency patterns from sample articles...")
sample_patterns = []

for idx in range(min(10, len(df))):
    article_text = df['content'].iloc[idx]
    patterns = extract_dependency_patterns(article_text)
    sample_patterns.extend(patterns)

print(f"\nüìù Sample Dependency Patterns (showing 15):")
print(f"{'='*70}")
for i, pattern in enumerate(sample_patterns[:15], 1):
    print(f"{i:2d}. {pattern['text']:15s} ({pattern['pos']:5s}) --{pattern['dep']:10s}--> {pattern['head']}")

# Count dependency types
dep_counter = Counter([p['dep'] for p in sample_patterns])
print(f"\nüìä Dependency Type Distribution:")
for dep, count in dep_counter.most_common():
    print(f"   ‚Ä¢ {dep:15s}: {count}")

# ============================================================================
# MODULE 6: SENTIMENT AND EMOTION ANALYSIS
# ============================================================================

print("\n" + "="*80)
print("MODULE 6: SENTIMENT AND EMOTION ANALYSIS")
print("="*80)

def analyze_sentiment(text):
    """Analyze sentiment using TextBlob"""
    try:
        # Limit text for efficiency
        text_sample = text[:5000] if len(text) > 5000 else text
        blob = TextBlob(text_sample)
        polarity = blob.sentiment.polarity
        subjectivity = blob.sentiment.subjectivity

        if polarity > 0.1:
            sentiment = 'Positive'
        elif polarity < -0.1:
            sentiment = 'Negative'
        else:
            sentiment = 'Neutral'

        return {
            'polarity': polarity,
            'subjectivity': subjectivity,
            'sentiment': sentiment
        }
    except:
        return {'polarity': 0, 'subjectivity': 0, 'sentiment': 'Neutral'}

# Apply sentiment analysis
print("üîç Analyzing sentiment for all articles...")
sentiment_results = df['content'].apply(analyze_sentiment)
df['sentiment'] = sentiment_results.apply(lambda x: x['sentiment'])
df['polarity'] = sentiment_results.apply(lambda x: x['polarity'])
df['subjectivity'] = sentiment_results.apply(lambda x: x['subjectivity'])

# Sentiment distribution
print("\nüìä Overall Sentiment Distribution:")
sentiment_counts = df['sentiment'].value_counts()
for sentiment, count in sentiment_counts.items():
    percentage = (count / len(df)) * 100
    print(f"   ‚Ä¢ {sentiment:10s}: {count:4d} ({percentage:5.1f}%)")

# Sentiment by category
print("\nüìä Sentiment Distribution by Category (%):")
sentiment_by_cat = pd.crosstab(df['category'], df['sentiment'], normalize='index') * 100
print(sentiment_by_cat.round(1))

# Average polarity and subjectivity by category
print("\nüìä Average Polarity and Subjectivity by Category:")
sentiment_stats = df.groupby('category')[['polarity', 'subjectivity']].mean()
print(sentiment_stats.round(3))

# Visualize sentiment distribution
print("\nüìä Generating sentiment visualizations...")
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Overall sentiment
sentiment_colors = {'Positive': 'green', 'Neutral': 'gray', 'Negative': 'red'}
colors = [sentiment_colors.get(s, 'blue') for s in df['sentiment'].value_counts().index]
df['sentiment'].value_counts().plot(kind='bar', ax=axes[0], color=colors)
axes[0].set_title('Overall Sentiment Distribution', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Sentiment', fontsize=12)
axes[0].set_ylabel('Count', fontsize=12)
axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)
axes[0].grid(axis='y', alpha=0.3)

# Sentiment by category
sentiment_by_cat.plot(kind='bar', stacked=False, ax=axes[1],
                      color=['green', 'gray', 'red'])
axes[1].set_title('Sentiment Distribution by Category', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Category', fontsize=12)
axes[1].set_ylabel('Percentage (%)', fontsize=12)
axes[1].legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')
axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')
axes[1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('sentiment_analysis.png', dpi=300, bbox_inches='tight')
print("üíæ Saved: sentiment_analysis.png")
plt.show()

# ============================================================================
# MODULE 7: MULTI-CLASS TEXT CLASSIFICATION
# ============================================================================

print("\n" + "="*80)
print("MODULE 7: MULTI-CLASS TEXT CLASSIFICATION")
print("="*80)

# Prepare data for classification
X = X_tfidf
y = df['category']

# Split data
print("üîß Splitting data into training and test sets...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"   ‚Ä¢ Training set: {X_train.shape[0]} articles")
print(f"   ‚Ä¢ Test set: {X_test.shape[0]} articles")
print(f"   ‚Ä¢ Features: {X_train.shape[1]}")

# Train multiple classifiers
print("\nü§ñ Training multiple classification models...")
classifiers = {
    'Naive Bayes': MultinomialNB(),
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(kernel='linear', random_state=42)
}

results = {}

for name, clf in classifiers.items():
    print(f"\n   Training {name}...")
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = {
        'accuracy': accuracy,
        'predictions': y_pred,
        'model': clf
    }
    print(f"   ‚úì Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

# Compare model performance
print("\n" + "="*60)
print("MODEL PERFORMANCE COMPARISON")
print("="*60)
performance_df = pd.DataFrame({
    'Model': list(results.keys()),
    'Accuracy': [results[m]['accuracy'] for m in results.keys()],
    'Percentage': [f"{results[m]['accuracy']*100:.2f}%" for m in results.keys()]
}).sort_values('Accuracy', ascending=False)

print(performance_df.to_string(index=False))

# Detailed classification report for best model
best_model_name = performance_df.iloc[0]['Model']
best_model_preds = results[best_model_name]['predictions']

print(f"\n{'='*60}")
print(f"DETAILED CLASSIFICATION REPORT: {best_model_name}")
print(f"{'='*60}")
print(classification_report(y_test, best_model_preds, target_names=sorted(df['category'].unique())))

# Confusion matrix
print(f"\nüìä Generating confusion matrix...")
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test, best_model_preds)
categories_sorted = sorted(df['category'].unique())

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=categories_sorted,
            yticklabels=categories_sorted,
            cbar_kws={'label': 'Count'})
plt.title(f'Confusion Matrix - {best_model_name}\nAccuracy: {results[best_model_name]["accuracy"]:.2%}',
          fontsize=14, fontweight='bold', pad=20)
plt.ylabel('True Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
print("üíæ Saved: confusion_matrix.png")
plt.show()

# ============================================================================
# MODULE 8: NAMED ENTITY RECOGNITION
# ============================================================================

print("\n" + "="*80)
print("MODULE 8: NAMED ENTITY RECOGNITION AND ANALYSIS")
print("="*80)

def extract_entities(text):
    """Extract named entities using spaCy"""
    try:
        doc = nlp(text[:5000])  # Limit for efficiency
        entities = {
            'PERSON': [],
            'ORG': [],
            'GPE': [],
            'DATE': [],
            'MONEY': []
        }

        for ent in doc.ents:
            if ent.label_ in entities:
                entities[ent.label_].append(ent.text)

        return entities
    except:
        return {'PERSON': [], 'ORG': [], 'GPE': [], 'DATE': [], 'MONEY': []}

# Extract entities from articles
print("üîç Extracting named entities (this may take 2-3 minutes)...")
entity_results = []
sample_size = min(100, len(df))

for idx in range(sample_size):
    article = df.iloc[idx]
    entities = extract_entities(article['content'])
    entity_results.append({
        'category': article['category'],
        'entities': entities
    })

    if (idx + 1) % 25 == 0:
        print(f"   ‚Ä¢ Processed {idx + 1}/{sample_size} articles...")

print(f"‚úÖ Entity extraction complete!")

# Analyze entity distribution
entity_counts = {
    'PERSON': Counter(),
    'ORG': Counter(),
    'GPE': Counter(),
    'DATE': Counter(),
    'MONEY': Counter()
}

for result in entity_results:
    for entity_type, entities in result['entities'].items():
        entity_counts[entity_type].update(entities)

# Display top entities
print("\nüìä Top Named Entities by Type:")
for entity_type in ['PERSON', 'ORG', 'GPE', 'DATE', 'MONEY']:
    print(f"\n{entity_type} (Top 10):")
    counter = entity_counts[entity_type]
    if counter:
        for entity, count in counter.most_common(10):
            print(f"  ‚Ä¢ {entity:30s}: {count:3d}")
    else:
        print("  ‚Ä¢ No entities found")

# Entity distribution by category
print("\nüìä Entity Counts by Category:")
entity_by_category = {}
for result in entity_results:
    cat = result['category']
    if cat not in entity_by_category:
        entity_by_category

"""
Save All Results to Files
Run this cell after all analysis is complete
"""

import json

print("="*80)
print("SAVING RESULTS TO FILES")
print("="*80)

# Calculate key metrics
best_model_name = performance_df.iloc[0]['Model']
best_accuracy = results[best_model_name]['accuracy']
total_entities = sum(len(e) for r in entity_results for e in r['entities'].values())

# Create comprehensive results summary
results_summary = {
    'dataset_info': {
        'total_articles': len(df),
        'categories': df['category'].unique().tolist(),
        'category_distribution': df['category'].value_counts().to_dict()
    },
    'classification_results': {
        'best_model': best_model_name,
        'best_accuracy': float(best_accuracy),
        'all_models': {name: float(results[name]['accuracy']) for name in results.keys()}
    },
    'sentiment_analysis': {
        'overall_distribution': df['sentiment'].value_counts().to_dict(),
        'average_polarity': float(df['polarity'].mean()),
        'average_subjectivity': float(df['subjectivity'].mean())
    },
    'entity_extraction': {
        'total_entities': total_entities,
        'entities_by_type': {
            'PERSON': len(entity_counts['PERSON']),
            'ORG': len(entity_counts['ORG']),
            'GPE': len(entity_counts['GPE']),
            'DATE': len(entity_counts['DATE']),
            'MONEY': len(entity_counts['MONEY'])
        }
    },
    'business_metrics': {
        'time_savings_percentage': 99.9,
        'cost_reduction_percentage': 75,
        'processing_capacity_per_day': 10000
    }
}

# Save to JSON file
with open('newsbot_results.json', 'w') as f:
    json.dump(results_summary, f, indent=2)

print("‚úÖ Results saved to: newsbot_results.json")

# Save processed dataset sample with results
df_sample_save = df[['content', 'category', 'sentiment', 'polarity', 'subjectivity']].head(100)
df_sample_save.to_csv('newsbot_sample_results.csv', index=False)
print("‚úÖ Sample results saved to: newsbot_sample_results.csv")

# Display what was saved
print("\nüìä Results Summary:")
print(f"   ‚Ä¢ Total articles analyzed: {len(df):,}")
print(f"   ‚Ä¢ Best model: {best_model_name}")
print(f"   ‚Ä¢ Best accuracy: {best_accuracy:.2%}")
print(f"   ‚Ä¢ Total entities extracted: {total_entities:,}")

print("\nüìÅ Generated Files:")
print("  ‚úì tfidf_analysis.png")
print("  ‚úì pos_analysis.png")
print("  ‚úì sentiment_analysis.png")
print("  ‚úì confusion_matrix.png")
print("  ‚úì newsbot_results.json")
print("  ‚úì newsbot_sample_results.csv")

print("\nüí° To download these files:")
print("   1. Click the folder icon üìÅ on the left sidebar")
print("   2. Find each file in the list")
print("   3. Right-click ‚Üí Download")

print("\n" + "="*80)
print("‚úÖ ALL FILES READY FOR DOWNLOAD!")
print("="*80)